<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://tusharganguli.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tusharganguli.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-02T21:19:54+00:00</updated><id>https://tusharganguli.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">What is a Random Variable? (Draft)</title><link href="https://tusharganguli.github.io/blog/2024/random-variable/" rel="alternate" type="text/html" title="What is a Random Variable? (Draft)"/><published>2024-11-17T12:22:11+00:00</published><updated>2024-11-17T12:22:11+00:00</updated><id>https://tusharganguli.github.io/blog/2024/random-variable</id><content type="html" xml:base="https://tusharganguli.github.io/blog/2024/random-variable/"><![CDATA[<h1 id="definition-of-a-random-variable">Definition of a Random Variable</h1> <p>According to Gubner [1], Note 2.1, A function X from \(\Omega\) into \(\mathbb{R}\) is a random variable if and only if,</p> \[\{\omega \in \Omega : X(\omega) \in B\} \in \mathcal{A}, \; \forall B \in \mathcal{B}\] <p>Here:</p> <ul> <li>\(\Omega\) : is the sample space.</li> <li>\(\omega\): outcome defined in the sample space.</li> <li>\(\mathcal{A}\): \(\sigma\)-algebra defined on subsets of sample space (\(\Omega\)).</li> <li>\(\mathcal{B}\): Borel \(\sigma\)-field is the smallest \(\sigma\)-algebra on \(\mathbb{R}\) containing all open subsets of \(\mathbb{R}\).</li> <li>\(B\): is the Borel set such that \(B \in \mathcal{B}\).</li> </ul> <p>where:</p> <ol> <li>\(\{\omega \in \Omega : X(\omega) \in B \}\): is the preimage of the Borel set \(B\) under the random variable \(X\). It is a subset of \(\Omega\), representing the set of outcomes \(\omega\) that map to values in \(B\) under \(X\).</li> <li>\(\in \mathcal{A}\): This subset must belong to the \(\sigma\)-algebra \(\mathcal{A}\) i.e., the preimage of any Borel set \(B\) is an event that can be measured (assigned a probability).</li> </ol> <p>If you understand the above definition in its entirety, then I congratulate you on your grasp of this fundamental concept of probability. If, on the other hand, you do not completely understand the above definition and struggle like me, then you are welcome to partake in this journey of understanding the nooks and crannies of <em>“how a random variable is defined”</em>.</p> <hr/> <h1 id="aim">Aim</h1> <p>The purpose of defining a random variable is to be able to study the probability associated with an experiment.</p> <hr/> <h1 id="sample-space-outcomes-and-events">Sample Space, Outcomes and Events</h1> <p>When we run an experiment it could produce many possible outcomes. Set of all such possible outcomes is the sample space (\(\Omega\)) of the experiment. A subset of the sample space, which is a collection of possible outcomes is called an event.</p> <h2 id="example">Example</h2> <p>Consider an experiment involving ten successive coin tosses.</p> <p><strong>Experiment</strong>: Ten successive coin tosses.</p> <p><strong>Outcomes</strong>: Each outcome is a \(10\) character string with each character is either a H or T.</p> \[\begin{align*} \omega_1 &amp;= \text{HHTTTHHTHH} \\ \omega_2 &amp;= \text{HTHHTTTHHH} \end{align*}\] <p><strong>Sample Space</strong>: Set of all such outcomes is the sample space \(\Omega\)</p> \[\Omega = \{ \text{HHHHHHHHHH}, \text{HTHHTTTHHH}, \text{TTTTTTTTTT}, \cdots \}\] <p>The total number of possible outcomes of the sample space is, \(\vert\Omega\vert = 2^{10} = 1024\).</p> <p><strong>Events</strong>: An event is a subset of the sample space. Some examples are:</p> <ul> <li> <p><strong>First Head Occurs on the 3rd Toss</strong>: The set of outcomes where the first Head appears on the 3rd toss:</p> \[E_1 = \{\omega \in \Omega : \omega = \text{TTHTTTTTTT or } \omega = \text{TTHHHHHHHH and so on.}\}\] </li> <li> <p><strong>Exactly 4 Heads in 10 Tosses</strong>: The set of outcomes with exactly 4 Heads: \(E_2 = \{\omega \in \Omega : \text{Number of H in } \omega = 4\}\)</p> </li> </ul> <p>The immediate question that arises is how all this fits into the defnition of a random variable. Now let’s formally define a random variable \(X\):</p> \[\begin{equation} X(\omega) : \Omega \rightarrow \mathbb{R} \end{equation}\] <p>Here:</p> <ul> <li>\(\Omega\) represents the sample space, which is the set of all possible outcomes of the random experiment.</li> <li>\(\mathbb{R}\) is the set of real numbers.</li> <li>\(X(\omega)\) represents the random variable that maps the outcome (\(\omega\)) to the real line (\(\mathbb{R}\)). It is also a real-valued function.</li> </ul> <p>A random variable acts as a bridge, translating outcomes (\(\omega\)) into numerical values (\(\mathbb{R}\)).</p> <ul> <li>The outcomes (\(\omega\)) that belong in the sample space (\(\Omega\)), correspond to the measure (distribution) that we want to study mathematically.</li> <li>The measure is already present in these outcomes (\(\omega\)) and we define a random variable (\(X\)) to map that measure to the Real line (\(\mathbb{R}\)).</li> </ul> <p><strong>Q. What is the purpose of this mapping?</strong></p> <p>The purpose of this mapping is to study the statistical properties of the random variable \(X\), which encapsulates the behavior of outcomes in the sample space. In statistical terms, the properties of a random variable are often described using measures like mean, variance, and higher-order moments. These measures summarize key aspects of the data, such as its central tendency (mean) and variability (variance). The moments of a random variable \(X\) like mean (\(\mathbb{E}[X]\)) and variance (\(\text{Var}(X)\)) provide critical insights into the distribution of \(X\). Higher-order moments (e.g., skewness and kurtosis) offer a more nuanced understanding of the behavior of \(X\). This mathematical representation allows us to gain significant insights into the behavior of a random variable through its moments.The exercise of creating a random variable is about defining a mapping from a sample space (\(\Omega\)) to the real numbers (\(\mathbb{R}\)).</p> <p>For example, if we define the random variable \(X\) as the number of heads in 2 successive coin flips, then \(X\)(No of Heads) \(= \{0,1,2\}\). The mapping defined by the random variable \(X\) induces a distribution from the underlying probability measure \(P\) on the sample space \(\Omega\). In this example we can say that \(X\) follows a Binomial distribution. This is because the random variable \(X\) transforms the distribution on the sample space \(\Omega\) into a distribution on the real line.</p> <hr/> <h1 id="types-of-random-variable">Types of Random Variable</h1> <p>Depending on the range of numerical values that the random variable \(X\) can take, a random variable is either a discrete random variable or a continuous random variable.</p> <ul> <li><strong>Discrete Random Variable</strong>: If the range of the random variable is finite or countably infinite.</li> <li><strong>Continuous Random Variable</strong>: If the range of the random variable is uncountably infinite.</li> </ul> <hr/> <h1 id="cardinality-of-a-set">Cardinality of a Set</h1> <p>The cardinality of a set refers to the size or number of elements in the set. Sets can have different cardinalities, which are broadly categorized as finite, countably infinite, and uncountably infinite.</p> <ul> <li> <p>A <strong>countably finite set</strong> is a set with a finite number of elements and can be put into a one-to-one correspondence with the first \(n\) positive integers, where \(n\) is finite.</p> <ul> <li><strong>Example</strong>: The set \(A = \{2, 4, 6, 8\}\) has 4 elements that correspond to the natural numbers \(\{1, 2, 3, 4\}\).</li> </ul> </li> <li> <p>A <strong>countably infinite set</strong> is a set with infinitely many elements, but these elements can still be put into a one-to-one correspondence with the set of natural numbers (\(\mathbb{N}\)).</p> <ul> <li><strong>Examples</strong>: <ul> <li>The set of all natural numbers: \(\mathbb{N} = \{1, 2, 3, 4, \dots\}\).</li> <li>The set of all integers: \(\mathbb{Z} = \{\dots, -2, -1, 0, 1, 2, \dots\}\). - \(\mathbb{Z}\) can be mapped to \(\mathbb{N}\) using: \(f(n) = \begin{cases} 2n &amp; \text{if } n &gt; 0 \\ -2n + 1 &amp; \text{if } n \leq 0 \end{cases}\)</li> <li>The set of even numbers, \(E = \{2, 4, 6, 8, \dots\}\) maps \(E \to \mathbb{N}\) with \(f(n) = 2n\).</li> </ul> </li> </ul> </li> <li> <p>A set is <strong>uncountably infinite</strong> if it contains infinitely many elements that cannot be put into a one-to-one correspondence with the set of natural numbers (\(\mathbb{N}\)). In other words, the set is too large to be enumerated, even in principle.</p> <ul> <li><strong>Example</strong>: The interval \([0, 1] \subset \mathbb{R}\) is uncountably infinite. This can be proven by Cantor’s diagonal argument, which demonstrates that no list can include all real numbers.</li> </ul> </li> </ul> <hr/> <h1 id="example-of-different-types-of-random-variable">Example of Different Types of Random Variable</h1> <ul> <li><strong>Discrete Random Variable - Countably Finite</strong>: <ul> <li>Experiment: Two successive rolls of a die.</li> <li>Random Variable: The sum of the two rolls is less than \(8\).</li> </ul> </li> <li><strong>Discrete Random Variable - Countably Infinite</strong>: <ul> <li>Experiment: Roll a single six-sided die repeatedly where the sample space for each roll is \(\Omega={1,2,3,4,5,6}\).</li> <li>Random Variable: The roll count on which \(6\) appears for the first time.</li> </ul> </li> <li><strong>Continuous Random Variable - Uncountably Infinite</strong>: <ul> <li>Experiment: Choosing a point \(a\) from the interval \([0,1]\).</li> <li>Random Variable: Associate the value \(a^2\) to such a point.</li> </ul> </li> </ul> <hr/> <h1 id="probability">Probability</h1> <ul> <li>The probability law assigns probabilities to event (\(A\)), which is a subset of the sample space (\(A \subseteq \Omega\)).</li> <li><strong>Remember</strong>: Random variable assigns values to outcomes.</li> </ul> <h2 id="example-1">Example</h2> <ul> <li>Experiment: Toss a coin three times.</li> <li>Sample Space: \(\Omega = \{\text{HHH},\text{HHT},\text{HTH},\text{HTT},\text{THH},\text{THT},\text{TTH},\text{TTT}\}\)</li> <li> <p>Random Variable: Let \(X\) represent the number of heads:</p> \[\begin{align*} X(\text{HHH}) &amp;= 3 \\ X(\text{HHT}) &amp;= X(\text{HTH}) = X(\text{THH}) = 2 \\ X(\text{HTT}) &amp;= X(\text{THT}) = X(\text{TTH}) = 1 \\ X(\text{TTT}) &amp;= 0 \end{align*}\] </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rv-480.webp 480w,/assets/img/rv-800.webp 800w,/assets/img/rv-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> X = {Number of Heads}. </div> <ul> <li>Let the event be, “The number of heads is 2”, which would be represented by: \(\{\text{HHT}, \text{HTH}, \text{THH}\} \subseteq \Omega\)</li> <li>The probability of an event would be calculated by summing the probabilities of the outcomes in that event. <ul> <li>As \(\{X=2\} = \{\text{HHT}, \text{HTH}, \text{THH}\}\), hence: \(P\{(X = 2)\} = P(\text{HHT}) + P(\text{HTH}) + P(\text{THH}) = \frac{1}{8} + \frac{1}{8} + \frac{1}{8} = \frac{3}{8}.\)</li> </ul> </li> </ul> <p>Hence, the random variable \(X\) creates a mapping from \(\Omega\) to \(\mathbb{R}\) and the probability law assigns probabilities to events such as \(\{X = x\}\), allowing us to compute the probability distribution of \(X\).</p> <hr/> <h1 id="probability-mass-function">Probability Mass Function</h1> <p>Now that we understand how random variables are defined and how probabilities are assigned, let’s tie all this up together.</p> <p>A random variable \(X\) is a function that maps each outcome \(\omega \in \Omega\) to a value \(x \in \mathbb{R}\). Different outcomes in the sample space \(\Omega\) can map to the same value \(x\) under \(X\).</p> <p>The probability mass function \(p_X(x)\) is defined as the probability of the event \(\{X = x\}\), which includes all outcomes in \(\Omega\) that result in \(X(\omega) = x\).</p> \[p_X(x) = P(\{X=x\})\] <p>Here, \(x\) is any possible value of \(X\), the probability mass of \(x\), denoted \(p_X(x)\), is the probability of the event \(\{X = x\}\) consisting of all outcomes that give rise to a value of \(X\) equal to \(x\). The previous example defines the random variable and demonstrates an example of how a specific event (\(P\{X=2\}\)) is assigned probability value.</p> <p><strong>Q. What does it mean when we say: The random variable \(X\) has a binomial distribution?</strong></p> <p>If \(X\) is a binomial random variable with parameters \(n\) and \(p\). Then the PMF of \(X\) is defined as:</p> \[P(\{X=k\}) = \binom{n}{x}p^x(1-p)^{n-x}\] <p>The above equation:</p> <ul> <li>Describes the probabilities of all possible values the random variable \(X\) can take.</li> <li>The event \(\{X=k\}\) is a subset of the sample space \(\Omega\) and contains all outcomes (\(\omega\)) for which \(X(\omega) = k\).</li> </ul> <hr/> <h1 id="continuous-random-variable">Continuous Random Variable</h1> <p>A continuous random variable can take an infinite number of possible values within a given range on the real line (\(\mathbb{R}\)). Continuous random variables can take on any value within an interval or collection of intervals on the real number line. For example, a random variable representing the height of people could be any real number within a plausible range (e.g., between 150 cm and 200 cm).</p> <p>Similar to a discrete random variable which is characterized by its PMF, a continuous random variable is described by a probability density function (PDF), denoted by \(f_{X}(x)\). The PDF gives the relative likelihood that the variable takes a particular value.</p> <p>Unlike a probability mass function (PMF) for discrete random variables, the PDF is not the probability of a specific value. The probability that the random variable takes on a specific value is always zero for a continuous variable. Rather, the PDF is used to compute the probability that the random variable lies within a certain interval.</p> \[\mathcal{P}(a \leq X \leq b) = \int_{a}^{b} f_{X}(x) dx\] <p>and can be interpreted as the area under the graph of the PDF.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pdf-480.webp 480w,/assets/img/pdf-800.webp 800w,/assets/img/pdf-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/pdf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Reference [2], Figure 3.1. PDF Illustration. </div> <hr/> <h1 id="continuous-models">Continuous Models</h1> <h1 id="references">References</h1> <p><strong>“If I have seen further, it is by standing on the shoulders of giants.”</strong><br/> <em>— Sir Isaac Newton</em></p> <ol> <li>Gubner, John A. <em>Probability and Random Processes for Electrical and Computer Engineers</em>. Cambridge University Press, 2006.</li> <li><a href="https://vfu.bg/en/e-Learning/Math--Bertsekas_Tsitsiklis_Introduction_to_probability.pdf">Introduction to Probability, by Dimitri P. Bertsekas and John N. Tsitsiklis</a></li> </ol>]]></content><author><name></name></author><category term="Probability"/><category term="randomvariable"/><category term="probability"/><summary type="html"><![CDATA[exposition on the definition of random variable.]]></summary></entry><entry><title type="html">End-To-End RAG Building</title><link href="https://tusharganguli.github.io/blog/2024/end-to-end-rag-building/" rel="alternate" type="text/html" title="End-To-End RAG Building"/><published>2024-10-29T21:15:01+00:00</published><updated>2024-10-29T21:15:01+00:00</updated><id>https://tusharganguli.github.io/blog/2024/end-to-end-rag-building</id><content type="html" xml:base="https://tusharganguli.github.io/blog/2024/end-to-end-rag-building/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Dataworkz RAG Builder: Evaluation Framework – Dataworkz – RAG as a Service</title><link href="https://tusharganguli.github.io/blog/2024/dataworkz-rag-builder-evaluation-framework-dataworkz-rag-as-a-service/" rel="alternate" type="text/html" title="Dataworkz RAG Builder: Evaluation Framework – Dataworkz – RAG as a Service"/><published>2024-08-12T00:00:00+00:00</published><updated>2024-08-12T00:00:00+00:00</updated><id>https://tusharganguli.github.io/blog/2024/dataworkz-rag-builder-evaluation-framework--dataworkz--rag-as-a-service</id><content type="html" xml:base="https://tusharganguli.github.io/blog/2024/dataworkz-rag-builder-evaluation-framework-dataworkz-rag-as-a-service/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Part 2 — Retrieval Augmented Generation using LlamaIndex</title><link href="https://tusharganguli.github.io/blog/2024/part-2retrieval-augmented-generation-using-llamaindex/" rel="alternate" type="text/html" title="Part 2 — Retrieval Augmented Generation using LlamaIndex"/><published>2024-02-22T20:44:12+00:00</published><updated>2024-02-22T20:44:12+00:00</updated><id>https://tusharganguli.github.io/blog/2024/part-2retrieval-augmented-generation-using-llamaindex</id><content type="html" xml:base="https://tusharganguli.github.io/blog/2024/part-2retrieval-augmented-generation-using-llamaindex/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Part 1 — Retrieval Augmented Generation</title><link href="https://tusharganguli.github.io/blog/2024/part-1retrieval-augmented-generation/" rel="alternate" type="text/html" title="Part 1 — Retrieval Augmented Generation"/><published>2024-02-18T22:10:34+00:00</published><updated>2024-02-18T22:10:34+00:00</updated><id>https://tusharganguli.github.io/blog/2024/part-1retrieval-augmented-generation</id><content type="html" xml:base="https://tusharganguli.github.io/blog/2024/part-1retrieval-augmented-generation/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Anomaly Detection - Time Series Data</title><link href="https://tusharganguli.github.io/blog/2023/anomaly_detection/" rel="alternate" type="text/html" title="Anomaly Detection - Time Series Data"/><published>2023-07-16T22:22:11+00:00</published><updated>2023-07-16T22:22:11+00:00</updated><id>https://tusharganguli.github.io/blog/2023/anomaly_detection</id><content type="html" xml:base="https://tusharganguli.github.io/blog/2023/anomaly_detection/"><![CDATA[<p>For time series data, methods studied and implemented were: Isolation forest Local Outlier Factor Autoencoders AutoRegressive Integrated Moving Average(ARIMA) Moving Average Z-score analysis.</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{::nomarkdown}

&lt;p&gt;Debug: Current Path = _posts/2024-07-16-anomaly_detection.md&lt;/p&gt;
&lt;p&gt;Debug: Jupyter Path = /jupyter/ad_timeseries.ipynb&lt;/p&gt;


  &lt;p&gt;Sorry, the notebook you are looking for does not exist.&lt;/p&gt;

{:/nomarkdown}
</code></pre></div></div>]]></content><author><name></name></author><category term="DataScience"/><category term="anomaly_detection"/><category term="data_analysis"/><category term="time_series"/><summary type="html"><![CDATA[Techniques for Anomaly Detection on Time Series Data]]></summary></entry></feed>